{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name and ID\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW04 Code\n",
    "\n",
    "\n",
    "You will complete the following notebook, as described in the PDF for Homework 04 (included in the download with the starter code).  You will submit:\n",
    "1. This notebook file, along with your COLLABORATORS.txt file, to the Gradescope link for code.\n",
    "2. A PDF of this notebook and all of its output, once it is completed, to the Gradescope link for the PDF.\n",
    "\n",
    "\n",
    "Please report any questions to the [class Piazza page](piazza.com/tufts/spring2021/comp135).\n",
    "\n",
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MLPClassifierWithSolverLBFGS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMLPClassifierWithSolverLBFGS\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MLPClassifierLBFGS\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mviz_tools_for_binary_classifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_pretty_probabilities_for_clf\n\u001b[0;32m     16\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'MLPClassifierWithSolverLBFGS'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from MLPClassifierWithSolverLBFGS import MLPClassifierLBFGS\n",
    "\n",
    "from viz_tools_for_binary_classifier import plot_pretty_probabilities_for_clf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_tr_N2 = np.loadtxt('data_xor/x_train.csv', skiprows=1, delimiter=',')\n",
    "x_te_N2 = np.loadtxt('data_xor/x_test.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "y_tr_N = np.loadtxt('data_xor/y_train.csv', skiprows=1, delimiter=',')\n",
    "y_te_N = np.loadtxt('data_xor/y_test.csv', skiprows=1, delimiter=',')\n",
    "\n",
    "assert x_tr_N2.shape[0] == y_tr_N.shape[0]\n",
    "assert x_te_N2.shape[0] == y_te_N.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: MLP size [2] with activation ReLU and L-BFGS solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_tr_N2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m      6\u001b[0m mlp_lbfgs \u001b[38;5;241m=\u001b[39m MLPClassifier(\n\u001b[0;32m      7\u001b[0m     hidden_layer_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m      8\u001b[0m     activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m warn_list:\n\u001b[1;32m---> 14\u001b[0m     mlp_lbfgs\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_tr_N2\u001b[49m, y_tr_N)\n\u001b[0;32m     15\u001b[0m elapsed_time_sec \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time_sec\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished LBFGS run \u001b[39m\u001b[38;5;132;01m%2d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m%6.1f\u001b[39;00m\u001b[38;5;124m sec | \u001b[39m\u001b[38;5;132;01m%3d\u001b[39;00m\u001b[38;5;124m iters | \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m | loss \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;241m1\u001b[39m, n_runs, elapsed_time_sec,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#len(mlp_lbfgs.loss_curve_),\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     mlp_lbfgs\u001b[38;5;241m.\u001b[39mn_iter_,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconverged   \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mlp_lbfgs\u001b[38;5;241m.\u001b[39mdid_converge \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNOT converged\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m     mlp_lbfgs\u001b[38;5;241m.\u001b[39mloss_))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_tr_N2' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO edit this block to run from 16 different random_states\n",
    "# Save each run's trained classifier object in a list\n",
    "\n",
    "n_runs = 16\n",
    "start_time_sec = time.time()\n",
    "mlp_lbfgs = MLPClassifier(\n",
    "    hidden_layer_sizes=[2],\n",
    "    activation='relu',\n",
    "    alpha=0.0001,\n",
    "    max_iter=200, tol=1e-6,\n",
    "    random_state=0,\n",
    "    )\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    mlp_lbfgs.fit(x_tr_N2, y_tr_N)\n",
    "elapsed_time_sec = time.time() - start_time_sec\n",
    "print('finished LBFGS run %2d/%d after %6.1f sec | %3d iters | %s | loss %.3f' % (\n",
    "    1, n_runs, elapsed_time_sec,\n",
    "    #len(mlp_lbfgs.loss_curve_),\n",
    "    mlp_lbfgs.n_iter_,\n",
    "    'converged   ' if mlp_lbfgs.did_converge else 'NOT converged',\n",
    "    mlp_lbfgs.loss_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 (a): Visualize probabilistic predictions in 2D feature space for ReLU + L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_grid = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "for subplot_id, mlp in enumerate(mlprelulbfgs_list):\n",
    "    cur_ax = ax_grid.flatten()[subplot_id]\n",
    "    plot_pretty_probabilities_for_clf(mlp, x_tr_N2, y_tr_N, ax=cur_ax)\n",
    "    \n",
    "plt.tight_layout()  \n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 (b): What fraction of runs reach 0 training error? What happens to the others? Describe how rapidly (or slowly) things seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: MLP size [2] with activation Logistic and L-BFGS solver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit this block to run 16 different random_state models with LOGISTIC activation\n",
    "\n",
    "# Save each run's trained classifier object in a list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 (a): Visualize probabilistic predictions in 2D feature space for Logistic Sigmoid + L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_grid = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "for subplot_id, mlp in enumerate(mlplogisticlbfgs_list):\n",
    "    cur_ax = ax_grid.flatten()[subplot_id]\n",
    "    plot_pretty_probabilities_for_clf(mlp, x_tr_N2, y_tr_N, ax=cur_ax)\n",
    "    \n",
    "plt.tight_layout()  \n",
    "plt.show()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 (b): What fraction of runs reach 0 training error? What happens to the others? Describe how rapidly (or slowly) things seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: MLP size [2] with activation ReLU and SGD solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit this block to do 16 different runs (each with different random_state value)\n",
    "# Save each run's trained classifier object in a list \n",
    "\n",
    "n_runs = 16\n",
    "start_time_sec = time.time()\n",
    "mlp_sgd = MLPClassifier(\n",
    "    hidden_layer_sizes=[2],\n",
    "    activation='relu',\n",
    "    alpha=0.0001,\n",
    "    max_iter=400, tol=1e-8,\n",
    "    random_state=0,\n",
    "    solver='sgd', batch_size=10,\n",
    "    learning_rate='adaptive', learning_rate_init=0.1, momentum=0.0,\n",
    "    )\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    mlp_sgd.fit(x_tr_N2, y_tr_N)\n",
    "mlp_sgd.did_converge = True if len(warn_list) == 0 else False\n",
    "elapsed_time_sec = time.time() - start_time_sec\n",
    "print('finished SGD run %2d/%d after %6.1f sec | %3d epochs | %s | loss %.3f' % (\n",
    "    1, n_runs, elapsed_time_sec,\n",
    "    len(mlp_sgd.loss_curve_),\n",
    "        'converged    ' if mlp_sgd.did_converge else 'NOT converged',\n",
    "        mlp_sgd.loss_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 (a): Visualize probabilistic predictions in 2D feature space for ReLU + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit to plot all 16 runs from above\n",
    "\n",
    "fig, ax_grid = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "plot_pretty_probabilities_for_clf(mlp_sgd, x_tr_N2, y_tr_N, ax=ax_grid[0,0])\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 (b): What fraction of runs reach 0 training error? What happens to the others? Describe how rapidly (or slowly) things seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 (c): What is most noticeably different between SGD with batch size 10 and the previous L-BFGS in part 1 (using the same ReLU activation function)?  Why, do you believe, these differences exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: MLP size [2] with activation Logistic and SGD solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit to do 16 runs of SGD, like in previous step, but with LOGISTIC activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(a): Visualize probabilistic predictions in 2D feature space for Logistic + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO edit to plot all 16 runs from previous step\n",
    "\n",
    "fig, ax_grid = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n",
    "plot_pretty_probabilities_for_clf(mlp_sgd, x_tr_N2, y_tr_N, ax=ax_grid[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 (b): What fraction of runs reach 0 training error? What happens to the others? Describe how rapidly (or slowly) things seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 (c): What is most noticeably different between SGD with batch size 10 and the previous L-BFGS runs in part 2 (using the same logistic activation function)?  Why, do you believe, these differences exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Plot SGD Loss Curves\n",
    "\n",
    "#### 5 (a): Plot Logistic and ReLU Loss Curved in 1 x 2 subplot grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_grid = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True, figsize=(8,5))\n",
    "\n",
    "# TODO plot 16 curves for each of the 1x2 settings of solver and activation    \n",
    "ax_grid[1,0].set_title('SGD ReLU')\n",
    "ax_grid[1,1].set_title('SGD Logistic')\n",
    "plt.ylim([0, 1.0]); # keep this y limit so it's easy to compare across plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 (b): Based on your iteration and timing data, which activation function seems easier to optimize, the ReLU or the Logistic Sigmoid?  Which requires most iterations in general?\n",
    "\n",
    "**Answer:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 (c): Are you convinced that one activation function is always easier to optimize? Suggest 3 additional experimental comparisons that would be informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
